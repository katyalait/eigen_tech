<!DOCTYPE html>
<html>
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=emulateIE7" />
    <title>Coverage for /home/kate/.local/lib/python3.6/site-packages/nltk/lm/__init__.py: 100%</title>
    <link rel="stylesheet" href="style.css" type="text/css">
    <script type="text/javascript" src="jquery.min.js"></script>
    <script type="text/javascript" src="jquery.hotkeys.js"></script>
    <script type="text/javascript" src="jquery.isonscreen.js"></script>
    <script type="text/javascript" src="coverage_html.js"></script>
    <script type="text/javascript">
        jQuery(document).ready(coverage.pyfile_ready);
    </script>
</head>
<body class="pyfile">
<div id="header">
    <div class="content">
        <h1>Coverage for <b>/home/kate/.local/lib/python3.6/site-packages/nltk/lm/__init__.py</b> :
            <span class="pc_cov">100%</span>
        </h1>
        <img id="keyboard_icon" src="keybd_closed.png" alt="Show keyboard shortcuts" />
        <h2 class="stats">
            5 statements &nbsp;
            <span class="run shortkey_r button_toggle_run">5 run</span>
            <span class="mis show_mis shortkey_m button_toggle_mis">0 missing</span>
            <span class="exc show_exc shortkey_x button_toggle_exc">0 excluded</span>
        </h2>
    </div>
</div>
<div class="help_panel">
    <img id="panel_icon" src="keybd_open.png" alt="Hide keyboard shortcuts" />
    <p class="legend">Hot-keys on this page</p>
    <div>
    <p class="keyhelp">
        <span class="key">r</span>
        <span class="key">m</span>
        <span class="key">x</span>
        <span class="key">p</span> &nbsp; toggle line displays
    </p>
    <p class="keyhelp">
        <span class="key">j</span>
        <span class="key">k</span> &nbsp; next/prev highlighted chunk
    </p>
    <p class="keyhelp">
        <span class="key">0</span> &nbsp; (zero) top of page
    </p>
    <p class="keyhelp">
        <span class="key">1</span> &nbsp; (one) first highlighted chunk
    </p>
    </div>
</div>
<div id="source">
    <p id="t1" class="pln"><span class="n"><a href="#t1">1</a></span><span class="t"><span class="com"># -*- coding: utf-8 -*-</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2" class="pln"><span class="n"><a href="#t2">2</a></span><span class="t"><span class="com"># Natural Language Toolkit: Language Models</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3" class="pln"><span class="n"><a href="#t3">3</a></span><span class="t"><span class="com">#</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4" class="pln"><span class="n"><a href="#t4">4</a></span><span class="t"><span class="com"># Copyright (C) 2001-2019 NLTK Project</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5" class="pln"><span class="n"><a href="#t5">5</a></span><span class="t"><span class="com"># Authors: Ilia Kurenkov &lt;ilia.kurenkov@gmail.com></span>&nbsp;</span><span class="r"></span></p>
    <p id="t6" class="pln"><span class="n"><a href="#t6">6</a></span><span class="t"><span class="com"># URL: &lt;http://nltk.org/</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7" class="pln"><span class="n"><a href="#t7">7</a></span><span class="t"><span class="com"># For license information, see LICENSE.TXT</span>&nbsp;</span><span class="r"></span></p>
    <p id="t8" class="run"><span class="n"><a href="#t8">8</a></span><span class="t"><span class="str">"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t9" class="pln"><span class="n"><a href="#t9">9</a></span><span class="t"><span class="str">NLTK Language Modeling Module.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t10" class="pln"><span class="n"><a href="#t10">10</a></span><span class="t"><span class="str">------------------------------</span>&nbsp;</span><span class="r"></span></p>
    <p id="t11" class="pln"><span class="n"><a href="#t11">11</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t12" class="pln"><span class="n"><a href="#t12">12</a></span><span class="t"><span class="str">Currently this module covers only ngram language models, but it should be easy</span>&nbsp;</span><span class="r"></span></p>
    <p id="t13" class="pln"><span class="n"><a href="#t13">13</a></span><span class="t"><span class="str">to extend to neural models.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t14" class="pln"><span class="n"><a href="#t14">14</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t15" class="pln"><span class="n"><a href="#t15">15</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t16" class="pln"><span class="n"><a href="#t16">16</a></span><span class="t"><span class="str">Preparing Data</span>&nbsp;</span><span class="r"></span></p>
    <p id="t17" class="pln"><span class="n"><a href="#t17">17</a></span><span class="t"><span class="str">==============</span>&nbsp;</span><span class="r"></span></p>
    <p id="t18" class="pln"><span class="n"><a href="#t18">18</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t19" class="pln"><span class="n"><a href="#t19">19</a></span><span class="t"><span class="str">Before we train our ngram models it is necessary to make sure the data we put in</span>&nbsp;</span><span class="r"></span></p>
    <p id="t20" class="pln"><span class="n"><a href="#t20">20</a></span><span class="t"><span class="str">them is in the right format.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t21" class="pln"><span class="n"><a href="#t21">21</a></span><span class="t"><span class="str">Let's say we have a text that is a list of sentences, where each sentence is</span>&nbsp;</span><span class="r"></span></p>
    <p id="t22" class="pln"><span class="n"><a href="#t22">22</a></span><span class="t"><span class="str">a list of strings. For simplicity we just consider a text consisting of</span>&nbsp;</span><span class="r"></span></p>
    <p id="t23" class="pln"><span class="n"><a href="#t23">23</a></span><span class="t"><span class="str">characters instead of words.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t24" class="pln"><span class="n"><a href="#t24">24</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t25" class="pln"><span class="n"><a href="#t25">25</a></span><span class="t"><span class="str">    >>> text = [['a', 'b', 'c'], ['a', 'c', 'd', 'c', 'e', 'f']]</span>&nbsp;</span><span class="r"></span></p>
    <p id="t26" class="pln"><span class="n"><a href="#t26">26</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t27" class="pln"><span class="n"><a href="#t27">27</a></span><span class="t"><span class="str">If we want to train a bigram model, we need to turn this text into bigrams.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t28" class="pln"><span class="n"><a href="#t28">28</a></span><span class="t"><span class="str">Here's what the first sentence of our text would look like if we use a function</span>&nbsp;</span><span class="r"></span></p>
    <p id="t29" class="pln"><span class="n"><a href="#t29">29</a></span><span class="t"><span class="str">from NLTK for this.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t30" class="pln"><span class="n"><a href="#t30">30</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t31" class="pln"><span class="n"><a href="#t31">31</a></span><span class="t"><span class="str">    >>> from nltk.util import bigrams</span>&nbsp;</span><span class="r"></span></p>
    <p id="t32" class="pln"><span class="n"><a href="#t32">32</a></span><span class="t"><span class="str">    >>> list(bigrams(text[0]))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t33" class="pln"><span class="n"><a href="#t33">33</a></span><span class="t"><span class="str">    [('a', 'b'), ('b', 'c')]</span>&nbsp;</span><span class="r"></span></p>
    <p id="t34" class="pln"><span class="n"><a href="#t34">34</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t35" class="pln"><span class="n"><a href="#t35">35</a></span><span class="t"><span class="str">Notice how "b" occurs both as the first and second member of different bigrams</span>&nbsp;</span><span class="r"></span></p>
    <p id="t36" class="pln"><span class="n"><a href="#t36">36</a></span><span class="t"><span class="str">but "a" and "c" don't? Wouldn't it be nice to somehow indicate how often sentences</span>&nbsp;</span><span class="r"></span></p>
    <p id="t37" class="pln"><span class="n"><a href="#t37">37</a></span><span class="t"><span class="str">start with "a" and end with "c"?</span>&nbsp;</span><span class="r"></span></p>
    <p id="t38" class="pln"><span class="n"><a href="#t38">38</a></span><span class="t"><span class="str">A standard way to deal with this is to add special "padding" symbols to the</span>&nbsp;</span><span class="r"></span></p>
    <p id="t39" class="pln"><span class="n"><a href="#t39">39</a></span><span class="t"><span class="str">sentence before splitting it into ngrams.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t40" class="pln"><span class="n"><a href="#t40">40</a></span><span class="t"><span class="str">Fortunately, NLTK also has a function for that, let's see what it does to the</span>&nbsp;</span><span class="r"></span></p>
    <p id="t41" class="pln"><span class="n"><a href="#t41">41</a></span><span class="t"><span class="str">first sentence.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t42" class="pln"><span class="n"><a href="#t42">42</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t43" class="pln"><span class="n"><a href="#t43">43</a></span><span class="t"><span class="str">    >>> from nltk.util import pad_sequence</span>&nbsp;</span><span class="r"></span></p>
    <p id="t44" class="pln"><span class="n"><a href="#t44">44</a></span><span class="t"><span class="str">    >>> list(pad_sequence(text[0],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t45" class="pln"><span class="n"><a href="#t45">45</a></span><span class="t"><span class="str">    ... pad_left=True,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t46" class="pln"><span class="n"><a href="#t46">46</a></span><span class="t"><span class="str">    ... left_pad_symbol="&lt;s>",</span>&nbsp;</span><span class="r"></span></p>
    <p id="t47" class="pln"><span class="n"><a href="#t47">47</a></span><span class="t"><span class="str">    ... pad_right=True,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t48" class="pln"><span class="n"><a href="#t48">48</a></span><span class="t"><span class="str">    ... right_pad_symbol="&lt;/s>",</span>&nbsp;</span><span class="r"></span></p>
    <p id="t49" class="pln"><span class="n"><a href="#t49">49</a></span><span class="t"><span class="str">    ... n=2))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t50" class="pln"><span class="n"><a href="#t50">50</a></span><span class="t"><span class="str">    ['&lt;s>', 'a', 'b', 'c', '&lt;/s>']</span>&nbsp;</span><span class="r"></span></p>
    <p id="t51" class="pln"><span class="n"><a href="#t51">51</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t52" class="pln"><span class="n"><a href="#t52">52</a></span><span class="t"><span class="str">Note the `n` argument, that tells the function we need padding for bigrams.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t53" class="pln"><span class="n"><a href="#t53">53</a></span><span class="t"><span class="str">Now, passing all these parameters every time is tedious and in most cases they</span>&nbsp;</span><span class="r"></span></p>
    <p id="t54" class="pln"><span class="n"><a href="#t54">54</a></span><span class="t"><span class="str">can be safely assumed as defaults anyway.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t55" class="pln"><span class="n"><a href="#t55">55</a></span><span class="t"><span class="str">Thus our module provides a convenience function that has all these arguments</span>&nbsp;</span><span class="r"></span></p>
    <p id="t56" class="pln"><span class="n"><a href="#t56">56</a></span><span class="t"><span class="str">already set while the other arguments remain the same as for `pad_sequence`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t57" class="pln"><span class="n"><a href="#t57">57</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t58" class="pln"><span class="n"><a href="#t58">58</a></span><span class="t"><span class="str">    >>> from nltk.lm.preprocessing import pad_both_ends</span>&nbsp;</span><span class="r"></span></p>
    <p id="t59" class="pln"><span class="n"><a href="#t59">59</a></span><span class="t"><span class="str">    >>> list(pad_both_ends(text[0], n=2))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t60" class="pln"><span class="n"><a href="#t60">60</a></span><span class="t"><span class="str">    ['&lt;s>', 'a', 'b', 'c', '&lt;/s>']</span>&nbsp;</span><span class="r"></span></p>
    <p id="t61" class="pln"><span class="n"><a href="#t61">61</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t62" class="pln"><span class="n"><a href="#t62">62</a></span><span class="t"><span class="str">Combining the two parts discussed so far we get the following preparation steps</span>&nbsp;</span><span class="r"></span></p>
    <p id="t63" class="pln"><span class="n"><a href="#t63">63</a></span><span class="t"><span class="str">for one sentence.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t64" class="pln"><span class="n"><a href="#t64">64</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t65" class="pln"><span class="n"><a href="#t65">65</a></span><span class="t"><span class="str">    >>> list(bigrams(pad_both_ends(text[0], n=2)))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t66" class="pln"><span class="n"><a href="#t66">66</a></span><span class="t"><span class="str">    [('&lt;s>', 'a'), ('a', 'b'), ('b', 'c'), ('c', '&lt;/s>')]</span>&nbsp;</span><span class="r"></span></p>
    <p id="t67" class="pln"><span class="n"><a href="#t67">67</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t68" class="pln"><span class="n"><a href="#t68">68</a></span><span class="t"><span class="str">To make our model more robust we could also train it on unigrams (single words)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t69" class="pln"><span class="n"><a href="#t69">69</a></span><span class="t"><span class="str">as well as bigrams, its main source of information.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t70" class="pln"><span class="n"><a href="#t70">70</a></span><span class="t"><span class="str">NLTK once again helpfully provides a function called `everygrams`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t71" class="pln"><span class="n"><a href="#t71">71</a></span><span class="t"><span class="str">While not the most efficient, it is conceptually simple.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t72" class="pln"><span class="n"><a href="#t72">72</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t73" class="pln"><span class="n"><a href="#t73">73</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t74" class="pln"><span class="n"><a href="#t74">74</a></span><span class="t"><span class="str">    >>> from nltk.util import everygrams</span>&nbsp;</span><span class="r"></span></p>
    <p id="t75" class="pln"><span class="n"><a href="#t75">75</a></span><span class="t"><span class="str">    >>> padded_bigrams = list(pad_both_ends(text[0], n=2))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t76" class="pln"><span class="n"><a href="#t76">76</a></span><span class="t"><span class="str">    >>> list(everygrams(padded_bigrams, max_len=2))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t77" class="pln"><span class="n"><a href="#t77">77</a></span><span class="t"><span class="str">    [('&lt;s>',),</span>&nbsp;</span><span class="r"></span></p>
    <p id="t78" class="pln"><span class="n"><a href="#t78">78</a></span><span class="t"><span class="str">     ('a',),</span>&nbsp;</span><span class="r"></span></p>
    <p id="t79" class="pln"><span class="n"><a href="#t79">79</a></span><span class="t"><span class="str">     ('b',),</span>&nbsp;</span><span class="r"></span></p>
    <p id="t80" class="pln"><span class="n"><a href="#t80">80</a></span><span class="t"><span class="str">     ('c',),</span>&nbsp;</span><span class="r"></span></p>
    <p id="t81" class="pln"><span class="n"><a href="#t81">81</a></span><span class="t"><span class="str">     ('&lt;/s>',),</span>&nbsp;</span><span class="r"></span></p>
    <p id="t82" class="pln"><span class="n"><a href="#t82">82</a></span><span class="t"><span class="str">     ('&lt;s>', 'a'),</span>&nbsp;</span><span class="r"></span></p>
    <p id="t83" class="pln"><span class="n"><a href="#t83">83</a></span><span class="t"><span class="str">     ('a', 'b'),</span>&nbsp;</span><span class="r"></span></p>
    <p id="t84" class="pln"><span class="n"><a href="#t84">84</a></span><span class="t"><span class="str">     ('b', 'c'),</span>&nbsp;</span><span class="r"></span></p>
    <p id="t85" class="pln"><span class="n"><a href="#t85">85</a></span><span class="t"><span class="str">     ('c', '&lt;/s>')]</span>&nbsp;</span><span class="r"></span></p>
    <p id="t86" class="pln"><span class="n"><a href="#t86">86</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t87" class="pln"><span class="n"><a href="#t87">87</a></span><span class="t"><span class="str">We are almost ready to start counting ngrams, just one more step left.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t88" class="pln"><span class="n"><a href="#t88">88</a></span><span class="t"><span class="str">During training and evaluation our model will rely on a vocabulary that</span>&nbsp;</span><span class="r"></span></p>
    <p id="t89" class="pln"><span class="n"><a href="#t89">89</a></span><span class="t"><span class="str">defines which words are "known" to the model.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t90" class="pln"><span class="n"><a href="#t90">90</a></span><span class="t"><span class="str">To create this vocabulary we need to pad our sentences (just like for counting</span>&nbsp;</span><span class="r"></span></p>
    <p id="t91" class="pln"><span class="n"><a href="#t91">91</a></span><span class="t"><span class="str">ngrams) and then combine the sentences into one flat stream of words.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t92" class="pln"><span class="n"><a href="#t92">92</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t93" class="pln"><span class="n"><a href="#t93">93</a></span><span class="t"><span class="str">    >>> from nltk.lm.preprocessing import flatten</span>&nbsp;</span><span class="r"></span></p>
    <p id="t94" class="pln"><span class="n"><a href="#t94">94</a></span><span class="t"><span class="str">    >>> list(flatten(pad_both_ends(sent, n=2) for sent in text))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t95" class="pln"><span class="n"><a href="#t95">95</a></span><span class="t"><span class="str">    ['&lt;s>', 'a', 'b', 'c', '&lt;/s>', '&lt;s>', 'a', 'c', 'd', 'c', 'e', 'f', '&lt;/s>']</span>&nbsp;</span><span class="r"></span></p>
    <p id="t96" class="pln"><span class="n"><a href="#t96">96</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t97" class="pln"><span class="n"><a href="#t97">97</a></span><span class="t"><span class="str">In most cases we want to use the same text as the source for both vocabulary</span>&nbsp;</span><span class="r"></span></p>
    <p id="t98" class="pln"><span class="n"><a href="#t98">98</a></span><span class="t"><span class="str">and ngram counts.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t99" class="pln"><span class="n"><a href="#t99">99</a></span><span class="t"><span class="str">Now that we understand what this means for our preprocessing, we can simply import</span>&nbsp;</span><span class="r"></span></p>
    <p id="t100" class="pln"><span class="n"><a href="#t100">100</a></span><span class="t"><span class="str">a function that does everything for us.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t101" class="pln"><span class="n"><a href="#t101">101</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t102" class="pln"><span class="n"><a href="#t102">102</a></span><span class="t"><span class="str">    >>> from nltk.lm.preprocessing import padded_everygram_pipeline</span>&nbsp;</span><span class="r"></span></p>
    <p id="t103" class="pln"><span class="n"><a href="#t103">103</a></span><span class="t"><span class="str">    >>> train, vocab = padded_everygram_pipeline(2, text)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t104" class="pln"><span class="n"><a href="#t104">104</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t105" class="pln"><span class="n"><a href="#t105">105</a></span><span class="t"><span class="str">So as to avoid re-creating the text in memory, both `train` and `vocab` are lazy</span>&nbsp;</span><span class="r"></span></p>
    <p id="t106" class="pln"><span class="n"><a href="#t106">106</a></span><span class="t"><span class="str">iterators. They are evaluated on demand at training time.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t107" class="pln"><span class="n"><a href="#t107">107</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t108" class="pln"><span class="n"><a href="#t108">108</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t109" class="pln"><span class="n"><a href="#t109">109</a></span><span class="t"><span class="str">Training</span>&nbsp;</span><span class="r"></span></p>
    <p id="t110" class="pln"><span class="n"><a href="#t110">110</a></span><span class="t"><span class="str">========</span>&nbsp;</span><span class="r"></span></p>
    <p id="t111" class="pln"><span class="n"><a href="#t111">111</a></span><span class="t"><span class="str">Having prepared our data we are ready to start training a model.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t112" class="pln"><span class="n"><a href="#t112">112</a></span><span class="t"><span class="str">As a simple example, let us train a Maximum Likelihood Estimator (MLE).</span>&nbsp;</span><span class="r"></span></p>
    <p id="t113" class="pln"><span class="n"><a href="#t113">113</a></span><span class="t"><span class="str">We only need to specify the highest ngram order to instantiate it.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t114" class="pln"><span class="n"><a href="#t114">114</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t115" class="pln"><span class="n"><a href="#t115">115</a></span><span class="t"><span class="str">    >>> from nltk.lm import MLE</span>&nbsp;</span><span class="r"></span></p>
    <p id="t116" class="pln"><span class="n"><a href="#t116">116</a></span><span class="t"><span class="str">    >>> lm = MLE(2)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t117" class="pln"><span class="n"><a href="#t117">117</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t118" class="pln"><span class="n"><a href="#t118">118</a></span><span class="t"><span class="str">This automatically creates an empty vocabulary...</span>&nbsp;</span><span class="r"></span></p>
    <p id="t119" class="pln"><span class="n"><a href="#t119">119</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t120" class="pln"><span class="n"><a href="#t120">120</a></span><span class="t"><span class="str">    >>> len(lm.vocab)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t121" class="pln"><span class="n"><a href="#t121">121</a></span><span class="t"><span class="str">    0</span>&nbsp;</span><span class="r"></span></p>
    <p id="t122" class="pln"><span class="n"><a href="#t122">122</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t123" class="pln"><span class="n"><a href="#t123">123</a></span><span class="t"><span class="str">... which gets filled as we fit the model.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t124" class="pln"><span class="n"><a href="#t124">124</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t125" class="pln"><span class="n"><a href="#t125">125</a></span><span class="t"><span class="str">    >>> lm.fit(train, vocab)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t126" class="pln"><span class="n"><a href="#t126">126</a></span><span class="t"><span class="str">    >>> print(lm.vocab)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t127" class="pln"><span class="n"><a href="#t127">127</a></span><span class="t"><span class="str">    &lt;Vocabulary with cutoff=1 unk_label='&lt;UNK>' and 9 items></span>&nbsp;</span><span class="r"></span></p>
    <p id="t128" class="pln"><span class="n"><a href="#t128">128</a></span><span class="t"><span class="str">    >>> len(lm.vocab)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t129" class="pln"><span class="n"><a href="#t129">129</a></span><span class="t"><span class="str">    9</span>&nbsp;</span><span class="r"></span></p>
    <p id="t130" class="pln"><span class="n"><a href="#t130">130</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t131" class="pln"><span class="n"><a href="#t131">131</a></span><span class="t"><span class="str">The vocabulary helps us handle words that have not occurred during training.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t132" class="pln"><span class="n"><a href="#t132">132</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t133" class="pln"><span class="n"><a href="#t133">133</a></span><span class="t"><span class="str">    >>> lm.vocab.lookup(text[0])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t134" class="pln"><span class="n"><a href="#t134">134</a></span><span class="t"><span class="str">    ('a', 'b', 'c')</span>&nbsp;</span><span class="r"></span></p>
    <p id="t135" class="pln"><span class="n"><a href="#t135">135</a></span><span class="t"><span class="str">    >>> lm.vocab.lookup(["aliens", "from", "Mars"])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t136" class="pln"><span class="n"><a href="#t136">136</a></span><span class="t"><span class="str">    ('&lt;UNK>', '&lt;UNK>', '&lt;UNK>')</span>&nbsp;</span><span class="r"></span></p>
    <p id="t137" class="pln"><span class="n"><a href="#t137">137</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t138" class="pln"><span class="n"><a href="#t138">138</a></span><span class="t"><span class="str">Moreover, in some cases we want to ignore words that we did see during training</span>&nbsp;</span><span class="r"></span></p>
    <p id="t139" class="pln"><span class="n"><a href="#t139">139</a></span><span class="t"><span class="str">but that didn't occur frequently enough, to provide us useful information.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t140" class="pln"><span class="n"><a href="#t140">140</a></span><span class="t"><span class="str">You can tell the vocabulary to ignore such words.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t141" class="pln"><span class="n"><a href="#t141">141</a></span><span class="t"><span class="str">To find out how that works, check out the docs for the `Vocabulary` class.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t142" class="pln"><span class="n"><a href="#t142">142</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t143" class="pln"><span class="n"><a href="#t143">143</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t144" class="pln"><span class="n"><a href="#t144">144</a></span><span class="t"><span class="str">Using a Trained Model</span>&nbsp;</span><span class="r"></span></p>
    <p id="t145" class="pln"><span class="n"><a href="#t145">145</a></span><span class="t"><span class="str">=====================</span>&nbsp;</span><span class="r"></span></p>
    <p id="t146" class="pln"><span class="n"><a href="#t146">146</a></span><span class="t"><span class="str">When it comes to ngram models the training boils down to counting up the ngrams</span>&nbsp;</span><span class="r"></span></p>
    <p id="t147" class="pln"><span class="n"><a href="#t147">147</a></span><span class="t"><span class="str">from the training corpus.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t148" class="pln"><span class="n"><a href="#t148">148</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t149" class="pln"><span class="n"><a href="#t149">149</a></span><span class="t"><span class="str">    >>> print(lm.counts)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t150" class="pln"><span class="n"><a href="#t150">150</a></span><span class="t"><span class="str">    &lt;NgramCounter with 2 ngram orders and 24 ngrams></span>&nbsp;</span><span class="r"></span></p>
    <p id="t151" class="pln"><span class="n"><a href="#t151">151</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t152" class="pln"><span class="n"><a href="#t152">152</a></span><span class="t"><span class="str">This provides a convenient interface to access counts for unigrams...</span>&nbsp;</span><span class="r"></span></p>
    <p id="t153" class="pln"><span class="n"><a href="#t153">153</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t154" class="pln"><span class="n"><a href="#t154">154</a></span><span class="t"><span class="str">    >>> lm.counts['a']</span>&nbsp;</span><span class="r"></span></p>
    <p id="t155" class="pln"><span class="n"><a href="#t155">155</a></span><span class="t"><span class="str">    2</span>&nbsp;</span><span class="r"></span></p>
    <p id="t156" class="pln"><span class="n"><a href="#t156">156</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t157" class="pln"><span class="n"><a href="#t157">157</a></span><span class="t"><span class="str">...and bigrams (in this case "a b")</span>&nbsp;</span><span class="r"></span></p>
    <p id="t158" class="pln"><span class="n"><a href="#t158">158</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t159" class="pln"><span class="n"><a href="#t159">159</a></span><span class="t"><span class="str">    >>> lm.counts[['a']]['b']</span>&nbsp;</span><span class="r"></span></p>
    <p id="t160" class="pln"><span class="n"><a href="#t160">160</a></span><span class="t"><span class="str">    1</span>&nbsp;</span><span class="r"></span></p>
    <p id="t161" class="pln"><span class="n"><a href="#t161">161</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t162" class="pln"><span class="n"><a href="#t162">162</a></span><span class="t"><span class="str">And so on. However, the real purpose of training a language model is to have it</span>&nbsp;</span><span class="r"></span></p>
    <p id="t163" class="pln"><span class="n"><a href="#t163">163</a></span><span class="t"><span class="str">score how probable words are in certain contexts.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t164" class="pln"><span class="n"><a href="#t164">164</a></span><span class="t"><span class="str">This being MLE, the model returns the item's relative frequency as its score.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t165" class="pln"><span class="n"><a href="#t165">165</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t166" class="pln"><span class="n"><a href="#t166">166</a></span><span class="t"><span class="str">    >>> lm.score("a")</span>&nbsp;</span><span class="r"></span></p>
    <p id="t167" class="pln"><span class="n"><a href="#t167">167</a></span><span class="t"><span class="str">    0.15384615384615385</span>&nbsp;</span><span class="r"></span></p>
    <p id="t168" class="pln"><span class="n"><a href="#t168">168</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t169" class="pln"><span class="n"><a href="#t169">169</a></span><span class="t"><span class="str">Items that are not seen during training are mapped to the vocabulary's</span>&nbsp;</span><span class="r"></span></p>
    <p id="t170" class="pln"><span class="n"><a href="#t170">170</a></span><span class="t"><span class="str">"unknown label" token. This is "&lt;UNK>" by default.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t171" class="pln"><span class="n"><a href="#t171">171</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t172" class="pln"><span class="n"><a href="#t172">172</a></span><span class="t"><span class="str">    >>> lm.score("&lt;UNK>") == lm.score("aliens")</span>&nbsp;</span><span class="r"></span></p>
    <p id="t173" class="pln"><span class="n"><a href="#t173">173</a></span><span class="t"><span class="str">    True</span>&nbsp;</span><span class="r"></span></p>
    <p id="t174" class="pln"><span class="n"><a href="#t174">174</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t175" class="pln"><span class="n"><a href="#t175">175</a></span><span class="t"><span class="str">Here's how you get the score for a word given some preceding context.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t176" class="pln"><span class="n"><a href="#t176">176</a></span><span class="t"><span class="str">For example we want to know what is the chance that "b" is preceded by "a".</span>&nbsp;</span><span class="r"></span></p>
    <p id="t177" class="pln"><span class="n"><a href="#t177">177</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t178" class="pln"><span class="n"><a href="#t178">178</a></span><span class="t"><span class="str">    >>> lm.score("b", ["a"])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t179" class="pln"><span class="n"><a href="#t179">179</a></span><span class="t"><span class="str">    0.5</span>&nbsp;</span><span class="r"></span></p>
    <p id="t180" class="pln"><span class="n"><a href="#t180">180</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t181" class="pln"><span class="n"><a href="#t181">181</a></span><span class="t"><span class="str">To avoid underflow when working with many small score values it makes sense to</span>&nbsp;</span><span class="r"></span></p>
    <p id="t182" class="pln"><span class="n"><a href="#t182">182</a></span><span class="t"><span class="str">take their logarithm.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t183" class="pln"><span class="n"><a href="#t183">183</a></span><span class="t"><span class="str">For convenience this can be done with the `logscore` method.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t184" class="pln"><span class="n"><a href="#t184">184</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t185" class="pln"><span class="n"><a href="#t185">185</a></span><span class="t"><span class="str">    >>> lm.logscore("a")</span>&nbsp;</span><span class="r"></span></p>
    <p id="t186" class="pln"><span class="n"><a href="#t186">186</a></span><span class="t"><span class="str">    -2.700439718141092</span>&nbsp;</span><span class="r"></span></p>
    <p id="t187" class="pln"><span class="n"><a href="#t187">187</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t188" class="pln"><span class="n"><a href="#t188">188</a></span><span class="t"><span class="str">Building on this method, we can also evaluate our model's cross-entropy and</span>&nbsp;</span><span class="r"></span></p>
    <p id="t189" class="pln"><span class="n"><a href="#t189">189</a></span><span class="t"><span class="str">perplexity with respect to sequences of ngrams.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t190" class="pln"><span class="n"><a href="#t190">190</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t191" class="pln"><span class="n"><a href="#t191">191</a></span><span class="t"><span class="str">    >>> test = [('a', 'b'), ('c', 'd')]</span>&nbsp;</span><span class="r"></span></p>
    <p id="t192" class="pln"><span class="n"><a href="#t192">192</a></span><span class="t"><span class="str">    >>> lm.entropy(test)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t193" class="pln"><span class="n"><a href="#t193">193</a></span><span class="t"><span class="str">    1.292481250360578</span>&nbsp;</span><span class="r"></span></p>
    <p id="t194" class="pln"><span class="n"><a href="#t194">194</a></span><span class="t"><span class="str">    >>> lm.perplexity(test)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t195" class="pln"><span class="n"><a href="#t195">195</a></span><span class="t"><span class="str">    2.449489742783178</span>&nbsp;</span><span class="r"></span></p>
    <p id="t196" class="pln"><span class="n"><a href="#t196">196</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t197" class="pln"><span class="n"><a href="#t197">197</a></span><span class="t"><span class="str">It is advisable to preprocess your test text exactly the same way as you did</span>&nbsp;</span><span class="r"></span></p>
    <p id="t198" class="pln"><span class="n"><a href="#t198">198</a></span><span class="t"><span class="str">the training text.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t199" class="pln"><span class="n"><a href="#t199">199</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t200" class="pln"><span class="n"><a href="#t200">200</a></span><span class="t"><span class="str">One cool feature of ngram models is that they can be used to generate text.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t201" class="pln"><span class="n"><a href="#t201">201</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t202" class="pln"><span class="n"><a href="#t202">202</a></span><span class="t"><span class="str">    >>> lm.generate(1, random_seed=3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t203" class="pln"><span class="n"><a href="#t203">203</a></span><span class="t"><span class="str">    '&lt;s>'</span>&nbsp;</span><span class="r"></span></p>
    <p id="t204" class="pln"><span class="n"><a href="#t204">204</a></span><span class="t"><span class="str">    >>> lm.generate(5, random_seed=3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t205" class="pln"><span class="n"><a href="#t205">205</a></span><span class="t"><span class="str">    ['&lt;s>', 'a', 'b', 'c', 'd']</span>&nbsp;</span><span class="r"></span></p>
    <p id="t206" class="pln"><span class="n"><a href="#t206">206</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t207" class="pln"><span class="n"><a href="#t207">207</a></span><span class="t"><span class="str">Provide `random_seed` if you want to consistently reproduce the same text all</span>&nbsp;</span><span class="r"></span></p>
    <p id="t208" class="pln"><span class="n"><a href="#t208">208</a></span><span class="t"><span class="str">other things being equal. Here we are using it to test the examples.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t209" class="pln"><span class="n"><a href="#t209">209</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t210" class="pln"><span class="n"><a href="#t210">210</a></span><span class="t"><span class="str">You can also condition your generation on some preceding text with the `context`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t211" class="pln"><span class="n"><a href="#t211">211</a></span><span class="t"><span class="str">argument.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t212" class="pln"><span class="n"><a href="#t212">212</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t213" class="pln"><span class="n"><a href="#t213">213</a></span><span class="t"><span class="str">    >>> lm.generate(5, text_seed=['c'], random_seed=3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t214" class="pln"><span class="n"><a href="#t214">214</a></span><span class="t"><span class="str">    ['&lt;/s>', 'c', 'd', 'c', 'd']</span>&nbsp;</span><span class="r"></span></p>
    <p id="t215" class="pln"><span class="n"><a href="#t215">215</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t216" class="pln"><span class="n"><a href="#t216">216</a></span><span class="t"><span class="str">Note that an ngram model is restricted in how much preceding context it can</span>&nbsp;</span><span class="r"></span></p>
    <p id="t217" class="pln"><span class="n"><a href="#t217">217</a></span><span class="t"><span class="str">take into account. For example, a trigram model can only condition its output</span>&nbsp;</span><span class="r"></span></p>
    <p id="t218" class="pln"><span class="n"><a href="#t218">218</a></span><span class="t"><span class="str">on 2 preceding words. If you pass in a 4-word context, the first two words</span>&nbsp;</span><span class="r"></span></p>
    <p id="t219" class="pln"><span class="n"><a href="#t219">219</a></span><span class="t"><span class="str">will be ignored.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t220" class="pln"><span class="n"><a href="#t220">220</a></span><span class="t"><span class="str">"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t221" class="pln"><span class="n"><a href="#t221">221</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t222" class="run"><span class="n"><a href="#t222">222</a></span><span class="t"><span class="key">from</span> <span class="nam">nltk</span><span class="op">.</span><span class="nam">lm</span><span class="op">.</span><span class="nam">models</span> <span class="key">import</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p id="t223" class="pln"><span class="n"><a href="#t223">223</a></span><span class="t">    <span class="nam">MLE</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t224" class="pln"><span class="n"><a href="#t224">224</a></span><span class="t">    <span class="nam">Lidstone</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t225" class="pln"><span class="n"><a href="#t225">225</a></span><span class="t">    <span class="nam">Laplace</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t226" class="pln"><span class="n"><a href="#t226">226</a></span><span class="t">    <span class="nam">WittenBellInterpolated</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t227" class="pln"><span class="n"><a href="#t227">227</a></span><span class="t">    <span class="nam">KneserNeyInterpolated</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t228" class="pln"><span class="n"><a href="#t228">228</a></span><span class="t"><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t229" class="run"><span class="n"><a href="#t229">229</a></span><span class="t"><span class="key">from</span> <span class="nam">nltk</span><span class="op">.</span><span class="nam">lm</span><span class="op">.</span><span class="nam">counter</span> <span class="key">import</span> <span class="nam">NgramCounter</span>&nbsp;</span><span class="r"></span></p>
    <p id="t230" class="run"><span class="n"><a href="#t230">230</a></span><span class="t"><span class="key">from</span> <span class="nam">nltk</span><span class="op">.</span><span class="nam">lm</span><span class="op">.</span><span class="nam">vocabulary</span> <span class="key">import</span> <span class="nam">Vocabulary</span>&nbsp;</span><span class="r"></span></p>
    <p id="t231" class="pln"><span class="n"><a href="#t231">231</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t232" class="run"><span class="n"><a href="#t232">232</a></span><span class="t"><span class="nam">__all__</span> <span class="op">=</span> <span class="op">[</span>&nbsp;</span><span class="r"></span></p>
    <p id="t233" class="pln"><span class="n"><a href="#t233">233</a></span><span class="t">    <span class="str">"Vocabulary"</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t234" class="pln"><span class="n"><a href="#t234">234</a></span><span class="t">    <span class="str">"NgramCounter"</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t235" class="pln"><span class="n"><a href="#t235">235</a></span><span class="t">    <span class="str">"MLE"</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t236" class="pln"><span class="n"><a href="#t236">236</a></span><span class="t">    <span class="str">"Lidstone"</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t237" class="pln"><span class="n"><a href="#t237">237</a></span><span class="t">    <span class="str">"Laplace"</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t238" class="pln"><span class="n"><a href="#t238">238</a></span><span class="t">    <span class="str">"WittenBellInterpolated"</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t239" class="pln"><span class="n"><a href="#t239">239</a></span><span class="t">    <span class="str">"KneserNeyInterpolated"</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t240" class="pln"><span class="n"><a href="#t240">240</a></span><span class="t"><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
</div>
<div id="footer">
    <div class="content">
        <p>
            <a class="nav" href="index.html">&#xab; index</a> &nbsp; &nbsp; <a class="nav" href="https://coverage.readthedocs.io">coverage.py v5.0.3</a>,
            created at 2020-01-27 12:54
        </p>
    </div>
</div>
</body>
</html>
